---
title: Auto-Encoding Variational Bayes
tags: [Python, CV, AI]
category: Computer-Vision
toc: true 
math: true
img_path: /assets/posts/vae/
---

## Auto Encoder

![Auto-Encoder 개요](ae-overview.png)

Auto Encoder(AE)는 데이터를 압축하고 복원하는 단순한 모델이다. Linear layer을 통해 데이터 크기를 줄이고 복원한다. Auto Encoder의 각 부분은 다음과 같은 용어를 사용한다.

- Encoder: 데이터를 압축하는 신경망 (파란 부분)
- Decoder: 데이터를 복원하는 신경망 (초록 부분)
- latent variable: 데이터가 압축된 벡터

다른 표현으로 Encoder를 Recognition model, Decoder를 Reconstruction model이라고 부르기도 한다. 구현도 어렵지 않다.

```python
class Autoencoder(nn.Module):
    def __init__(self):
        super(Autoencoder, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(in_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, latent_dim),
            nn.ReLU(),
        )
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, in_dim),
            nn.Tanh(),
        )
```

이를 활용하면 이미지 노이즈를 제거할 수 있다. 전체 코드: [Github](https://github.com/denev6/deep-learning-codes/blob/main/models/auto_encoder.ipynb).

![Auto-encoder 노이즈 제거](ae-noise.png)

입력을 노이즈 있는 이미지, 정답을 노이즈 없는 이미지로 두고 학습하면 노이즈를 제거하는 모델이 학습된다. 같은 맥락으로 워터마크를 제거하는 모델도 만들 수 있다.

## Variational AE 개요

"[Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114)"에서 소개된 모델로, latent variable을 확률 분포에서 샘플링한다.

![VAE 개요](vae-overview.png)

Encoder가 latent vector를 출력하는 대신, 평균(mean)과 표준편차(std)를 출력한다. 평균과 표준편차를 이용해 Gaussian 분포를 생성하고 latent variable을 샘플링한다. 즉, Gaussian 분포 $N(\mu ,\sigma^2)$에 대해 Encoder는 $\mu$와 $\sigma$를 생성하도록 학습한다. 샘플링된 latent variable $z$는 Decoder의 입력이 된다. 조금 더 깊이 들어가보자.

![VAE 수식 표현](vae-math.png)

확률 분포를 생성하고 샘플링하는 과정을 수식으로 표현해보자.

- $p_{\theta}(x)$: 풀려는 문제. 올바른 $x$를 생성해낼 확률.
- $p_{\theta}(x|z)$: Decoder. latent $z$로부터 $x$가 나올 확률.
- $p_{\theta}(z|x)$: Encoder. 입력 $x$로부터 latent $z$가 나올 확률.
  - $q_{\phi}(z|x)$: $p_{\theta}(z|x)$의 근삿값.

먼저, Encoder는 입력 $x$가 주어졌을 때 $z$를 출력한다. 그런데 우리는 $x$에 대응하는 $z$를 알지 못한다. 따라서 $p_{\theta}(z|x)$도 구할 수 없다. 대신 Encoder를 학습시켜 $p_{\theta}$에 근사하는 $q_{\phi}$를 구한다. 다시 말해, Encoder를 학습하는 과정은 파라미터 $\phi$를 학습시켜 $q_{\phi}$가 $p_{\theta}$에 가까워지도록 한다.

Decoder는 Encoder로부터 $z$가 주어졌을 때 $x$를 출력한다. 따라서 $p_{\theta}(x|z)$로 표현할 수 있다.

> 참고로 $p(x)$는 Encoder + Decoder를 나타내는 식이 아니다. 다만, 정의한 문제 $p(x)=p(z)p(x|z)$를 풀기 위해 추론에 Encoder, Decoder 구조를 활용하는 것일 뿐이다.
{: .info }

## Stochastic Gradient Variational Bayes

Loss function을 구하기 위해 한 단계씩 알아보자. $\log p_{\theta}(x)$는 KL-divergence와 Lower bound로 표현할 수 있다.

$$\log p_{\theta}(\mathbf{x}^{(i)}) = D_{KL} \left( q_{\phi}(\mathbf{z} | \mathbf{x}^{(i)}) \parallel p_{\theta}(\mathbf{z} | \mathbf{x}^{(i)}) \right) + \mathcal{L}(\theta, \phi; \mathbf{x}^{(i)})$$

$$\log p_{\theta}(\mathbf{x}^{(i)}) = KL + \mathcal{L}(\theta, \phi; \mathbf{x}^{(i)})$$

KL-divergence는 항상 양수이기 때문에 다음과 같은 부등식이 성립한다.

$$\log p_{\theta}(\mathbf{x}^{(i)}) \geq \mathcal{L}(\theta, \phi; \mathbf{x}^{(i)})$$

따라서 $\log p_{\theta}(\mathbf{x}^{(i)})$를 최대화하기 위해서는 $\mathcal{L}(\theta, \phi; \mathbf{x}^{(i)})$을 최대화해야 하고, 다시 말해 $- \mathcal{L}(\theta, \phi; \mathbf{x}^{(i)})$를 최소화해야 한다.

이 식을 다시 작성하면 다음과 같다.

$$- \mathcal{L}(\theta, \phi; \mathbf{x}^{(i)}) = D_{KL} \left( q_{\phi}(\mathbf{z} | \mathbf{x}^{(i)}) \parallel p_{\theta}(\mathbf{z}) \right) - \mathbb{E}_{q_{\phi}(\mathbf{z} | \mathbf{x}^{(i)})} \left[ \log p_{\theta}(\mathbf{x}^{(i)} | \mathbf{z}) \right]$$

여기서 식의 우변을 Regularization + Reconstruction로 표현할 수 있다.

- **Regularization Loss**: Encoder가 주어진 $x$에 대해 $z$를 잘 생성하는지
- **Reconstruction Loss**: Decoder가 주어진 $z$에 대해 $x$를 잘 생성하는지

즉, VAE의 Loss function은 Lower bound로부터 파생된다. 이 Loss는 Encoder와 Decoder에 대한 Loss를 더한 값이다. 자세한 과정은 논문 2.2와 2.3에 기록되어 있다.

### reparameterization trick

앞서 설명했듯 VAE는 $z$는 Gaussian 분포에서 샘플링한다. 평균을 $\mu$, 표준편차를 $\sigma$라 할 때,

$$z^{(i,l)}\sim q_{\phi}(z|x^{(i)})$$

$$z^{(i,l)} = \mu^{(i)} + \sigma^{(i)} \odot \epsilon^{(l)}$$

$\epsilon\sim N(0,1)$는 랜덤한 작은 값이다. 이 식을 통해 $z$를 Gaussian 분포로부터 샘플링할 수 있다.

```python
epsilon = randn_like(std)
z = mu + std * epsilon
```

### Loss Function 정의

위 Loss는 일반화된 모습의 Loss function이다. 구현을 위해서는 구체적인 식을 정의해야 한다.

$$\log q_{\phi}(z|x^{(i)})=\log N(z;\mu^{(i)}),\sigma^{2(i)}I$$

이를 바탕으로 Loss function을 다시 정의하면,

$$- \mathcal{L}(\theta, \phi; \mathbf{x}^{(i)}) \simeq - \frac{1}{2} \sum_{j=1}^{J} \left( 1 + \log \left( (\sigma_{j}^{(i)})^2 \right) - (\mu_{j}^{(i)})^2 - (\sigma_{j}^{(i)})^2 \right) - \frac{1}{L} \sum_{l=1}^{L} \log p_{\theta} (\mathbf{x}^{(i)} | \mathbf{z}^{(i,l)})$$

이 식은 Gaussian 분포를 바탕으로 Regularization Loss를 구체적으로 정의했다. 두번째 항인 Reconstruction Loss는 negative log-likelihood다. 따라서, Binary Cross Entropy로 정의할 수 있다.

```python
def loss(x, x_reconstructed, mu, std):
    # Regularization Loss
    kl_div = -0.5 * sum(1 + log(std.pow(2)) - mu.pow(2) - std.pow(2))
    # Reconstruction Loss
    recon_loss = binary_cross_entropy(x_reconstructed, x)
    return kl_div + recon_loss
```

## Pytorch 구현

전체 구현은 [Github: VAE](https://github.com/denev6/deep-learning-codes/blob/main/models/VAE/train_vae.ipynb)에서 확인할 수 있다.

```python
class VAE(nn.Module):
    def __init__(self, input_dim, hidden_dim, latent_dim):
        super(VAE, self).__init__()
        self.encoder = Encoder(input_dim, hidden_dim, latent_dim)
        self.decoder = Decoder(latent_dim, hidden_dim, input_dim)

    def forward(self, x):
        mu, logvar = self.encoder(x)

        # Reparameterization trick
        std = torch.exp(0.5 * logvar)
        epsilon = torch.randn_like(std)
        z = mu + std * epsilon

        x_recon = self.decoder(z)
        return x_recon, mu, logvar
```

구현 시에는 표준편차 $\sigma$ 대신 $\log \sigma^2$인 `logvar`를 반환한다.

```python
def loss(x, x_recon, mu, logvar):
    recon_loss = nn.functional.binary_cross_entropy(x_recon, x)
    kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    return recon_loss + kl_div
```

이렇게하면 Loss function도 `logvar`에 대해 재정의할 수 있다.

MNIST 데이터셋을 이용해 학습하면 다음과 같이 잘 생성해 낸다.

![MNIST 결과](mnist-result.png)

### 시각화

2차원 latent space를 시각화했다. 코드: [Github](https://github.com/denev6/deep-learning-codes/blob/main/models/VAE/vae_visual.py)

![2D latent visualization](2d-latent.png)

2차원 latent space, 즉 2개의 Gaussian 분포를 생성하도록 Encoder를 학습시켰다. $z$를 \[-3, 3\] 범위에 대해 Decoder에 입력했다. Gaussian 분포는 \[-3, 3\] 범위에 약 99%의 값이 있으므로 latent space 대부분을 시각화할 수 있다.

시각화한 이미지를 통해 샘플링된 $z$에 따라 필요한 이미지를 찾아내는 과정을 볼 수 있다.
